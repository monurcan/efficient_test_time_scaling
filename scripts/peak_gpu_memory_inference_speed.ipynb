{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c19103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/monka/tta-vlm-new/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93693c0c1f4e45219f0267a5146ce440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    _attn_implementation=\"flash_attention_2\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bef561",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Can you describe this image?\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b8018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPUMemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.peak_memory = 0\n",
    "        self.monitoring = False\n",
    "        self.monitor_thread = None\n",
    "    \n",
    "    def get_gpu_memory_mb(self):\n",
    "        \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        return 0\n",
    "    \n",
    "    def monitor_memory(self):\n",
    "        \"\"\"Monitor GPU memory usage in a separate thread\"\"\"\n",
    "        while self.monitoring:\n",
    "            current_memory = self.get_gpu_memory_mb()\n",
    "            if current_memory > self.peak_memory:\n",
    "                self.peak_memory = current_memory\n",
    "            time.sleep(0.001)  # Check every 1ms\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start monitoring GPU memory\"\"\"\n",
    "        self.peak_memory = self.get_gpu_memory_mb()\n",
    "        self.monitoring = True\n",
    "        self.monitor_thread = threading.Thread(target=self.monitor_memory)\n",
    "        self.monitor_thread.start()\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop monitoring and return peak memory usage\"\"\"\n",
    "        self.monitoring = False\n",
    "        if self.monitor_thread:\n",
    "            self.monitor_thread.join()\n",
    "        return self.peak_memory\n",
    "\n",
    "# Initialize the memory monitor\n",
    "memory_monitor = GPUMemoryMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc375f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU memory usage: 4321.53 MB\n",
      "\n",
      "=== Performance Metrics ===\n",
      "Generated text: User:\n",
      "\n",
      "\n",
      "\n",
      "Can you describe this image?\n",
      "Assistant: The image depicts a close-up view of a bee on a pink flower. The bee is positioned in the center of the flower, with its body prominently visible. The bee appears to be engaged in the act of pollination, as it is surrounded by the petals of the flower. The flower itself is vibrant and has a\n",
      "\n",
      "=== Timing ===\n",
      "Inference time: 2.8372 seconds\n",
      "Tokens per second: 22.56 tokens/sec\n",
      "\n",
      "=== Memory Usage ===\n",
      "Initial GPU memory: 4321.53 MB\n",
      "Peak GPU memory: 4585.85 MB\n",
      "Final GPU memory: 4330.09 MB\n",
      "Memory increase during generation: 264.32 MB\n",
      "\n",
      "=== GPU Memory Details ===\n",
      "GPU memory allocated: 4330.09 MB\n",
      "GPU memory reserved: 4908.00 MB\n",
      "GPU memory cached: 4908.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work3/monka/.CACHE/tmp/ipykernel_3697456/3272880413.py:56: FutureWarning: `torch.cuda.memory_cached` has been renamed to `torch.cuda.memory_reserved`\n",
      "  print(f\"GPU memory cached: {torch.cuda.memory_cached() / 1024 / 1024:.2f} MB\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Clear GPU cache before measurement\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Get initial memory usage\n",
    "initial_memory = memory_monitor.get_gpu_memory_mb()\n",
    "print(f\"Initial GPU memory usage: {initial_memory:.2f} MB\")\n",
    "\n",
    "# Start memory monitoring\n",
    "memory_monitor.start_monitoring()\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate text\n",
    "generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=64)\n",
    "\n",
    "# Stop timing\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# Stop memory monitoring\n",
    "peak_memory = memory_monitor.stop_monitoring()\n",
    "\n",
    "# Decode the generated text\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "# Calculate memory usage\n",
    "memory_increase = peak_memory - initial_memory\n",
    "final_memory = memory_monitor.get_gpu_memory_mb()\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n=== Performance Metrics ===\")\n",
    "print(f\"Generated text: {generated_texts[0]}\")\n",
    "print(f\"\\n=== Timing ===\")\n",
    "print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "print(f\"Tokens per second: {64/inference_time:.2f} tokens/sec\")\n",
    "\n",
    "print(f\"\\n=== Memory Usage ===\")\n",
    "print(f\"Initial GPU memory: {initial_memory:.2f} MB\")\n",
    "print(f\"Peak GPU memory: {peak_memory:.2f} MB\")\n",
    "print(f\"Final GPU memory: {final_memory:.2f} MB\")\n",
    "print(f\"Memory increase during generation: {memory_increase:.2f} MB\")\n",
    "\n",
    "# Additional GPU memory info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\n=== GPU Memory Details ===\")\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024 / 1024:.2f} MB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_cached() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3d093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Different Batch Sizes (10 runs each for averaging) ===\n",
      "\n",
      "Testing batch size: 1 (10 runs)\n",
      "  Batch shape: torch.Size([1, 1102]) (batch_size=1, seq_len=1102)\n",
      "  Run 1/10... ‚úì 2.196s\n",
      "  Run 2/10... ‚úì 2.188s\n",
      "  Run 3/10... ‚úì 2.185s\n",
      "  Run 4/10... ‚úì 2.179s\n",
      "  Run 5/10... ‚úì 2.161s\n",
      "  Run 6/10... ‚úì 2.177s\n",
      "  Run 7/10... ‚úì 2.189s\n",
      "  Run 8/10... ‚úì 2.174s\n",
      "  Run 9/10... ‚úì 2.184s\n",
      "  Run 10/10... ‚úì 2.175s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 1 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 2 (10 runs)\n",
      "  Batch shape: torch.Size([2, 1103]) (batch_size=2, seq_len=1103)\n",
      "  Run 1/10... ‚úì 2.433s\n",
      "  Run 2/10... ‚úì 2.334s\n",
      "  Run 3/10... ‚úì 2.322s\n",
      "  Run 4/10... ‚úì 2.333s\n",
      "  Run 5/10... ‚úì 2.322s\n",
      "  Run 6/10... ‚úì 2.348s\n",
      "  Run 7/10... ‚úì 2.339s\n",
      "  Run 8/10... ‚úì 2.350s\n",
      "  Run 9/10... ‚úì 2.341s\n",
      "  Run 10/10... ‚úì 2.321s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 2 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 4 (10 runs)\n",
      "  Batch shape: torch.Size([4, 1103]) (batch_size=4, seq_len=1103)\n",
      "  Run 1/10... ‚úì 2.559s\n",
      "  Run 2/10... ‚úì 2.502s\n",
      "  Run 3/10... ‚úì 2.503s\n",
      "  Run 4/10... ‚úì 2.498s\n",
      "  Run 5/10... ‚úì 2.499s\n",
      "  Run 6/10... ‚úì 2.512s\n",
      "  Run 7/10... ‚úì 2.519s\n",
      "  Run 8/10... ‚úì 2.515s\n",
      "  Run 9/10... ‚úì 2.516s\n",
      "  Run 10/10... ‚úì 2.517s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 4 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 8 (10 runs)\n",
      "  Batch shape: torch.Size([8, 1103]) (batch_size=8, seq_len=1103)\n",
      "  Run 1/10... ‚úì 4.111s\n",
      "  Run 2/10... ‚úì 4.017s\n",
      "  Run 3/10... ‚úì 4.024s\n",
      "  Run 4/10... ‚úì 4.021s\n",
      "  Run 5/10... ‚úì 4.037s\n",
      "  Run 6/10... ‚úì 4.038s\n",
      "  Run 7/10... ‚úì 4.025s\n",
      "  Run 8/10... ‚úì 4.016s\n",
      "  Run 9/10... ‚úì 4.021s\n",
      "  Run 10/10... ‚úì 4.030s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 8 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 16 (10 runs)\n",
      "  Batch shape: torch.Size([16, 1104]) (batch_size=16, seq_len=1104)\n",
      "  Run 1/10... ‚úì 5.213s\n",
      "  Run 2/10... ‚úì 5.206s\n",
      "  Run 3/10... ‚úì 5.206s\n",
      "  Run 4/10... ‚úì 5.216s\n",
      "  Run 5/10... ‚úì 5.257s\n",
      "  Run 6/10... ‚úì 5.260s\n",
      "  Run 7/10... ‚úì 5.248s\n",
      "  Run 8/10... ‚úì 5.211s\n",
      "  Run 9/10... ‚úì 5.223s\n",
      "  Run 10/10... ‚úì 5.229s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 16 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 32 (10 runs)\n",
      "  Batch shape: torch.Size([32, 1104]) (batch_size=32, seq_len=1104)\n",
      "  Run 1/10... ‚úì 8.529s\n",
      "  Run 2/10... ‚úì 8.564s\n",
      "  Run 3/10... ‚úì 8.585s\n",
      "  Run 4/10... ‚úì 8.598s\n",
      "  Run 5/10... ‚úì 8.609s\n",
      "  Run 6/10... ‚úì 8.603s\n",
      "  Run 7/10... ‚úì 8.597s\n",
      "  Run 8/10... ‚úì 8.610s\n",
      "  Run 9/10... ‚úì 8.604s\n",
      "  Run 10/10... ‚úì 8.601s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 32 completed successfully (avg from 10 runs)\n",
      "\n",
      "Testing batch size: 64 (10 runs)\n",
      "  Batch shape: torch.Size([64, 1104]) (batch_size=64, seq_len=1104)\n",
      "  Run 1/10... ‚úì 16.150s\n",
      "  Run 2/10... ‚úì 16.279s\n",
      "  Run 3/10... ‚úì 16.286s\n",
      "  Run 4/10... ‚úì 16.297s\n",
      "  Run 5/10... ‚úì 16.298s\n",
      "  Run 6/10... ‚úì 16.311s\n",
      "  Run 7/10... ‚úì 16.295s\n",
      "  Run 8/10... ‚úì 16.290s\n",
      "  Run 9/10... ‚úì 16.287s\n",
      "  Run 10/10... ‚úì 16.288s\n",
      "  Completed 10/10 successful runs\n",
      "‚úì Batch size 64 completed successfully (avg from 10 runs)\n",
      "\n",
      "=== Performance Results Table (Averaged from 10 runs) ===\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|   Batch Size | Avg Total Time   | Avg Time/Sample   |   Avg Tokens/Sec | Avg Initial Mem   | Avg Peak Mem   | Avg Memory Inc   | Avg Final Mem   | Success Rate   |\n",
      "+==============+==================+===================+==================+===================+================+==================+=================+================+\n",
      "|            1 | 2.1808s          | 2.1808s           |            29.35 | 4341.1 MB         | 4594.8 MB      | 253.7 MB         | 4341.1 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|            2 | 2.3441s          | 1.1721s           |            54.61 | 4352.1 MB         | 4856.9 MB      | 504.7 MB         | 4352.1 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|            4 | 2.5139s          | 0.6285s           |           101.84 | 4374.1 MB         | 5385.2 MB      | 1011.1 MB        | 4374.1 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|            8 | 4.0339s          | 0.5042s           |           126.93 | 4418.3 MB         | 6463.1 MB      | 2044.8 MB        | 4418.3 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|           16 | 5.2270s          | 0.3267s           |           195.91 | 4506.0 MB         | 8659.1 MB      | 4153.1 MB        | 4506.0 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|           32 | 8.5899s          | 0.2684s           |           238.42 | 4682.9 MB         | 12870.3 MB     | 8187.4 MB        | 4682.9 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "|           64 | 16.2781s         | 0.2543s           |           251.63 | 5033.7 MB         | 21519.8 MB     | 16486.1 MB       | 5033.7 MB       | 10/10          |\n",
      "+--------------+------------------+-------------------+------------------+-------------------+----------------+------------------+-----------------+----------------+\n",
      "\n",
      "=== Key Insights ===\n",
      "‚Ä¢ Best throughput: 64 batch size with 251.63 tokens/sec\n",
      "‚Ä¢ Lowest latency per sample: 64 batch size with 0.2543s per sample\n",
      "‚Ä¢ Memory usage scales from 253.7 MB to 16486.1 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import random\n",
    "\n",
    "def create_batch_inputs(batch_size):\n",
    "    \"\"\"Create batched inputs by processing multiple messages together with proper padding\"\"\"\n",
    "    # Set padding to left for proper batching\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    # Create batch_size number of the same message format\n",
    "    batch_messages = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        batch_messages.append([\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"},\n",
    "                    {\"type\": \"text\", \"text\": f\"Can you describe this image? please {i} {random.randint(0, 100)}\"},\n",
    "                ]\n",
    "            }\n",
    "        ])\n",
    "    \n",
    "    # Process each conversation separately then combine\n",
    "    all_inputs = []\n",
    "    for single_messages in batch_messages:\n",
    "        single_input = processor.apply_chat_template(\n",
    "            single_messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        all_inputs.append(single_input)\n",
    "    \n",
    "    # Now combine all inputs with proper padding\n",
    "    # Get the maximum sequence length\n",
    "    max_len = max(inp['input_ids'].size(1) for inp in all_inputs)\n",
    "    \n",
    "    # Pad all sequences to the same length (left padding)\n",
    "    batch_input_ids = []\n",
    "    batch_attention_masks = []\n",
    "    batch_pixel_values = []\n",
    "    \n",
    "    for inp in all_inputs:\n",
    "        input_ids = inp['input_ids'].squeeze(0)  # Remove batch dim\n",
    "        attention_mask = inp['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Left pad\n",
    "        pad_length = max_len - len(input_ids)\n",
    "        if pad_length > 0:\n",
    "            # Pad with tokenizer.pad_token_id\n",
    "            pad_token_id = processor.tokenizer.pad_token_id if processor.tokenizer.pad_token_id is not None else processor.tokenizer.eos_token_id\n",
    "            input_ids = torch.cat([torch.full((pad_length,), pad_token_id, dtype=input_ids.dtype), input_ids])\n",
    "            attention_mask = torch.cat([torch.zeros(pad_length, dtype=attention_mask.dtype), attention_mask])\n",
    "        \n",
    "        batch_input_ids.append(input_ids)\n",
    "        batch_attention_masks.append(attention_mask)\n",
    "        if 'pixel_values' in inp:\n",
    "            batch_pixel_values.append(inp['pixel_values'])\n",
    "    \n",
    "    # Stack into batches\n",
    "    batch_inputs = {\n",
    "        'input_ids': torch.stack(batch_input_ids).to(model.device),\n",
    "        'attention_mask': torch.stack(batch_attention_masks).to(model.device),\n",
    "    }\n",
    "    \n",
    "    if batch_pixel_values:\n",
    "        batch_inputs['pixel_values'] = torch.cat(batch_pixel_values, dim=0).to(model.device, dtype=torch.bfloat16)\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "def measure_batch_performance(batch_size, max_new_tokens=64, num_runs=10):\n",
    "    \"\"\"Measure performance for a given batch size with multiple runs for averaging\"\"\"\n",
    "    print(f\"Testing batch size: {batch_size} ({num_runs} runs)\")\n",
    "    \n",
    "    # Create batch inputs once\n",
    "    try:\n",
    "        batch_inputs = create_batch_inputs(batch_size)\n",
    "        \n",
    "        # Check sequence length before proceeding\n",
    "        seq_len = batch_inputs['input_ids'].shape[1]\n",
    "        batch_size_actual = batch_inputs['input_ids'].shape[0]\n",
    "        print(f\"  Batch shape: {batch_inputs['input_ids'].shape} (batch_size={batch_size_actual}, seq_len={seq_len})\")\n",
    "        \n",
    "        if seq_len > 16384:  # Model's max sequence length\n",
    "            print(f\"  ‚ö†Ô∏è  Sequence length {seq_len} exceeds model limit of 16384\")\n",
    "            print(f\"  Skipping batch size {batch_size} due to sequence length limit\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating batch inputs for size {batch_size}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Store results from all runs\n",
    "    all_results = []\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        print(f\"  Run {run + 1}/{num_runs}...\", end=\" \")\n",
    "        \n",
    "        # Clear GPU cache before each measurement\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Get initial memory usage\n",
    "        initial_memory = memory_monitor.get_gpu_memory_mb()\n",
    "        \n",
    "        # Start memory monitoring\n",
    "        memory_monitor.start_monitoring()\n",
    "        \n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Generate text\n",
    "            generated_ids = model.generate(**batch_inputs, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "            \n",
    "            # Stop timing\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Stop memory monitoring\n",
    "            peak_memory = memory_monitor.stop_monitoring()\n",
    "            \n",
    "            # Calculate metrics for this run\n",
    "            memory_increase = peak_memory - initial_memory\n",
    "            final_memory = memory_monitor.get_gpu_memory_mb()\n",
    "            total_tokens = batch_size * max_new_tokens\n",
    "            tokens_per_second = total_tokens / inference_time\n",
    "            time_per_sample = inference_time / batch_size\n",
    "            \n",
    "            run_result = {\n",
    "                'inference_time': inference_time,\n",
    "                'time_per_sample': time_per_sample,\n",
    "                'tokens_per_second': tokens_per_second,\n",
    "                'initial_memory': initial_memory,\n",
    "                'peak_memory': peak_memory,\n",
    "                'memory_increase': memory_increase,\n",
    "                'final_memory': final_memory\n",
    "            }\n",
    "            \n",
    "            all_results.append(run_result)\n",
    "            print(f\"‚úì {inference_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "            memory_monitor.stop_monitoring()\n",
    "            continue\n",
    "    \n",
    "    if not all_results:\n",
    "        print(f\"  All runs failed for batch size {batch_size}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate averages\n",
    "    print(f\"  Completed {len(all_results)}/{num_runs} successful runs\")\n",
    "    \n",
    "    avg_result = {\n",
    "        'batch_size': batch_size,\n",
    "        'inference_time': sum(r['inference_time'] for r in all_results) / len(all_results),\n",
    "        'time_per_sample': sum(r['time_per_sample'] for r in all_results) / len(all_results),\n",
    "        'tokens_per_second': sum(r['tokens_per_second'] for r in all_results) / len(all_results),\n",
    "        'initial_memory': sum(r['initial_memory'] for r in all_results) / len(all_results),\n",
    "        'peak_memory': sum(r['peak_memory'] for r in all_results) / len(all_results),\n",
    "        'memory_increase': sum(r['memory_increase'] for r in all_results) / len(all_results),\n",
    "        'final_memory': sum(r['final_memory'] for r in all_results) / len(all_results),\n",
    "        'num_successful_runs': len(all_results)\n",
    "    }\n",
    "    \n",
    "    return avg_result\n",
    "\n",
    "# Test different batch sizes - now with proper batching (separate sequences)\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]  # Can test larger batches now since sequences aren't concatenated\n",
    "results = []\n",
    "\n",
    "print(\"=== Testing Different Batch Sizes (10 runs each for averaging) ===\\n\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    result = measure_batch_performance(batch_size, num_runs=10)\n",
    "    if result is not None:\n",
    "        results.append(result)\n",
    "        print(f\"‚úì Batch size {batch_size} completed successfully (avg from {result['num_successful_runs']} runs)\")\n",
    "    else:\n",
    "        print(f\"‚úó Batch size {batch_size} failed\")\n",
    "        break  # Stop if we hit memory limits\n",
    "    print()\n",
    "\n",
    "# Create and display results table\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Format the table\n",
    "    formatted_df = df.copy()\n",
    "    formatted_df['inference_time'] = formatted_df['inference_time'].apply(lambda x: f\"{x:.4f}s\")\n",
    "    formatted_df['time_per_sample'] = formatted_df['time_per_sample'].apply(lambda x: f\"{x:.4f}s\")\n",
    "    formatted_df['tokens_per_second'] = formatted_df['tokens_per_second'].apply(lambda x: f\"{x:.2f}\")\n",
    "    formatted_df['initial_memory'] = formatted_df['initial_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "    formatted_df['peak_memory'] = formatted_df['peak_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "    formatted_df['memory_increase'] = formatted_df['memory_increase'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "    formatted_df['final_memory'] = formatted_df['final_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "    formatted_df['num_successful_runs'] = formatted_df['num_successful_runs'].apply(lambda x: f\"{x}/10\")\n",
    "    \n",
    "    # Rename columns for better display\n",
    "    formatted_df.columns = [\n",
    "        'Batch Size', 'Avg Total Time', 'Avg Time/Sample', 'Avg Tokens/Sec', \n",
    "        'Avg Initial Mem', 'Avg Peak Mem', 'Avg Memory Inc', 'Avg Final Mem', 'Success Rate'\n",
    "    ]\n",
    "    \n",
    "    print(\"=== Performance Results Table (Averaged from 10 runs) ===\")\n",
    "    print(tabulate(formatted_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    # Also create a summary with key metrics\n",
    "    print(\"\\n=== Key Insights ===\")\n",
    "    print(f\"‚Ä¢ Best throughput: {df.loc[df['tokens_per_second'].idxmax(), 'batch_size']} batch size with {df['tokens_per_second'].max():.2f} tokens/sec\")\n",
    "    print(f\"‚Ä¢ Lowest latency per sample: {df.loc[df['time_per_sample'].idxmin(), 'batch_size']} batch size with {df['time_per_sample'].min():.4f}s per sample\")\n",
    "    print(f\"‚Ä¢ Memory usage scales from {df['memory_increase'].min():.1f} MB to {df['memory_increase'].max():.1f} MB\")\n",
    "    \n",
    "else:\n",
    "    print(\"No successful results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c3996b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying results: Length mismatch: Expected axis has 9 elements, new values have 8 elements\n",
      "Results might not be available yet.\n"
     ]
    }
   ],
   "source": [
    "# Let's check if we have results and display them if the previous cell output was truncated\n",
    "try:\n",
    "    if 'results' in locals() and results:\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Format the table\n",
    "        formatted_df = df.copy()\n",
    "        formatted_df['inference_time'] = formatted_df['inference_time'].apply(lambda x: f\"{x:.4f}s\")\n",
    "        formatted_df['time_per_sample'] = formatted_df['time_per_sample'].apply(lambda x: f\"{x:.4f}s\")\n",
    "        formatted_df['tokens_per_second'] = formatted_df['tokens_per_second'].apply(lambda x: f\"{x:.2f}\")\n",
    "        formatted_df['initial_memory'] = formatted_df['initial_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "        formatted_df['peak_memory'] = formatted_df['peak_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "        formatted_df['memory_increase'] = formatted_df['memory_increase'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "        formatted_df['final_memory'] = formatted_df['final_memory'].apply(lambda x: f\"{x:.1f} MB\")\n",
    "        \n",
    "        # Rename columns for better display\n",
    "        formatted_df.columns = [\n",
    "            'Batch Size', 'Total Time', 'Time/Sample', 'Tokens/Sec', \n",
    "            'Initial Mem', 'Peak Mem', 'Memory Increase', 'Final Mem'\n",
    "        ]\n",
    "        \n",
    "        print(\"=== Performance Results Table ===\")\n",
    "        print(tabulate(formatted_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "        \n",
    "        # Also create a summary with key metrics\n",
    "        print(\"\\n=== Key Insights ===\")\n",
    "        print(f\"‚Ä¢ Best throughput: Batch size {df.loc[df['tokens_per_second'].idxmax(), 'batch_size']} with {df['tokens_per_second'].max():.2f} tokens/sec\")\n",
    "        print(f\"‚Ä¢ Lowest latency per sample: Batch size {df.loc[df['time_per_sample'].idxmin(), 'batch_size']} with {df['time_per_sample'].min():.4f}s per sample\")\n",
    "        print(f\"‚Ä¢ Memory usage scales from {df['memory_increase'].min():.1f} MB to {df['memory_increase'].max():.1f} MB\")\n",
    "        print(f\"‚Ä¢ Peak memory usage ranges from {df['peak_memory'].min():.1f} MB to {df['peak_memory'].max():.1f} MB\")\n",
    "        \n",
    "        # Show efficiency metrics\n",
    "        print(f\"\\n=== Efficiency Analysis ===\")\n",
    "        for _, row in df.iterrows():\n",
    "            efficiency = row['batch_size'] / row['inference_time']\n",
    "            print(f\"‚Ä¢ Batch {int(row['batch_size'])}: {efficiency:.2f} samples/second, {row['memory_increase']:.1f} MB memory increase\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results found. Please run the previous cell first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error displaying results: {e}\")\n",
    "    print(\"Results might not be available yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5bcd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPACT PERFORMANCE SUMMARY (Averaged from 10 runs each) ===\n",
      "Batch | Avg Total Time | Avg Tokens/Sec | Avg Memory Inc | Avg Peak Memory | Success\n",
      "------|----------------|----------------|----------------|-----------------|--------\n",
      "    1 |        2.1808s |         29.35 |         253.7MB |         4594.8MB | 10/10\n",
      "    2 |        2.3441s |         54.61 |         504.7MB |         4856.9MB | 10/10\n",
      "    4 |        2.5139s |        101.84 |        1011.1MB |         5385.2MB | 10/10\n",
      "    8 |        4.0339s |        126.93 |        2044.8MB |         6463.1MB | 10/10\n",
      "   16 |        5.2270s |        195.91 |        4153.1MB |         8659.1MB | 10/10\n",
      "   32 |        8.5899s |        238.42 |        8187.4MB |        12870.3MB | 10/10\n",
      "   64 |       16.2781s |        251.63 |       16486.1MB |        21519.8MB | 10/10\n",
      "\n",
      "üöÄ BEST THROUGHPUT: Batch 64 = 251.6 tokens/sec\n",
      "‚ö° BEST LATENCY: Batch 64 = 0.2543s per sample\n",
      "üíæ MEMORY RANGE: 253.7MB - 16486.1MB increase\n"
     ]
    }
   ],
   "source": [
    "# Compact summary of results\n",
    "if 'results' in locals() and results:\n",
    "    print(\"=== COMPACT PERFORMANCE SUMMARY (Averaged from 10 runs each) ===\")\n",
    "    print(\"Batch | Avg Total Time | Avg Tokens/Sec | Avg Memory Inc | Avg Peak Memory | Success\")\n",
    "    print(\"------|----------------|----------------|----------------|-----------------|--------\")\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"{result['batch_size']:5d} | {result['inference_time']:13.4f}s | {result['tokens_per_second']:13.2f} | {result['memory_increase']:13.1f}MB | {result['peak_memory']:14.1f}MB | {result['num_successful_runs']:2d}/10\")\n",
    "    \n",
    "    # Best performance summary\n",
    "    df = pd.DataFrame(results)\n",
    "    best_throughput_idx = df['tokens_per_second'].idxmax()\n",
    "    best_latency_idx = df['time_per_sample'].idxmin()\n",
    "    \n",
    "    print(f\"\\nüöÄ BEST THROUGHPUT: Batch {int(df.loc[best_throughput_idx, 'batch_size'])} = {df.loc[best_throughput_idx, 'tokens_per_second']:.1f} tokens/sec\")\n",
    "    print(f\"‚ö° BEST LATENCY: Batch {int(df.loc[best_latency_idx, 'batch_size'])} = {df.loc[best_latency_idx, 'time_per_sample']:.4f}s per sample\")\n",
    "    print(f\"üíæ MEMORY RANGE: {df['memory_increase'].min():.1f}MB - {df['memory_increase'].max():.1f}MB increase\")\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da168821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPACT PERFORMANCE SUMMARY (Averaged from 10 runs each) ===\n",
      "Batch | Avg Total Time | Avg Tokens/Sec | Avg Memory Inc | Avg Peak Memory | Success\n",
      "------|----------------|----------------|----------------|-----------------|--------\n",
      "    1 |        1.0849s |         58.99 |         255.0MB |         4619.5MB | 10/10\n",
      "    2 |        1.2191s |        104.99 |         517.6MB |         4893.2MB | 10/10\n",
      "    4 |        1.3328s |        192.08 |        1003.0MB |         5400.5MB | 10/10\n",
      "    8 |        2.3268s |        220.05 |        1981.8MB |         6423.5MB | 10/10\n",
      "   16 |        3.2665s |        313.48 |        4078.6MB |         8608.1MB | 10/10\n",
      "   32 |        5.7176s |        358.20 |        8109.9MB |        12816.2MB | 10/10\n",
      "\n",
      "üöÄ BEST THROUGHPUT: Batch 32 = 358.2 tokens/sec\n",
      "‚ö° BEST LATENCY: Batch 32 = 0.1787s per sample\n",
      "üíæ MEMORY RANGE: 255.0MB - 8109.9MB increase\n"
     ]
    }
   ],
   "source": [
    "# h100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPACT PERFORMANCE SUMMARY (Averaged from 10 runs each) ===\n",
      "Batch | Avg Total Time | Avg Tokens/Sec | Avg Memory Inc | Avg Peak Memory | Success\n",
      "------|----------------|----------------|----------------|-----------------|--------\n",
      "    1 |        2.1808s |         29.35 |         253.7MB |         4594.8MB | 10/10\n",
      "    2 |        2.3441s |         54.61 |         504.7MB |         4856.9MB | 10/10\n",
      "    4 |        2.5139s |        101.84 |        1011.1MB |         5385.2MB | 10/10\n",
      "    8 |        4.0339s |        126.93 |        2044.8MB |         6463.1MB | 10/10\n",
      "   16 |        5.2270s |        195.91 |        4153.1MB |         8659.1MB | 10/10\n",
      "   32 |        8.5899s |        238.42 |        8187.4MB |        12870.3MB | 10/10\n",
      "   64 |       16.2781s |        251.63 |       16486.1MB |        21519.8MB | 10/10\n",
      "\n",
      "üöÄ BEST THROUGHPUT: Batch 64 = 251.6 tokens/sec\n",
      "‚ö° BEST LATENCY: Batch 64 = 0.2543s per sample\n",
      "üíæ MEMORY RANGE: 253.7MB - 16486.1MB increase\n"
     ]
    }
   ],
   "source": [
    "# a100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
