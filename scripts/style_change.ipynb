{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Adjust the search root if needed; here we use the current working directory.\n",
    "search_root = \"/dtu/blackhole/00/215456/tta-vlm/benchmark_results/\"\n",
    "\n",
    "# Find folders starting with 'augmented_inputs_'\n",
    "pattern = os.path.join(search_root, \"augmented_inputs_*\")\n",
    "folders = [folder for folder in glob.glob(pattern) if os.path.isdir(folder)]\n",
    "input_images_paths = []\n",
    "\n",
    "for folder in folders:\n",
    "    # Find all .png files in the folder\n",
    "    png_pattern = os.path.join(folder, \"*.png\")\n",
    "    png_files = glob.glob(png_pattern)\n",
    "    \n",
    "    # Skip the folder if no png files are found\n",
    "    if not png_files:\n",
    "        continue\n",
    "\n",
    "    # Extract the numeric part of the filename (assuming the file is named like 'number.png')\n",
    "    # and build a list of tuples: (index, absolute_path)\n",
    "    indexed_files = []\n",
    "    for f in png_files:\n",
    "        basename = os.path.basename(f)\n",
    "        name, ext = os.path.splitext(basename)\n",
    "        try:\n",
    "            index = int(name)\n",
    "            indexed_files.append((index, os.path.abspath(f)))\n",
    "        except ValueError:\n",
    "            # Skip files that do not have an integer as their base name.\n",
    "            continue\n",
    "\n",
    "    if not indexed_files:\n",
    "        continue\n",
    "\n",
    "    # Find the file with the maximum index\n",
    "    max_index, max_path = max(indexed_files, key=lambda item: item[0])\n",
    "    input_images_paths.append(max_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForImage2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "from IPython.display import display\n",
    "\n",
    "pipeline = AutoPipelineForImage2Image.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\" # \"stabilityai/stable-diffusion-3.5-large\", #torch_dtype=torch.bfloat16, use_safetensors=True\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()\n",
    "#pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = '/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1736425391.8340986/4.png'\n",
    "# url = \"/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1737465403.7203906/9.png\"\n",
    "for index, url in enumerate(input_images_paths):\n",
    "    init_image = load_image(url)\n",
    "    crop_box = (0, 0, 512, init_image.height)\n",
    "    init_image = init_image.crop(crop_box)\n",
    "    prompt = \"realistic image\"\n",
    "\n",
    "    image = pipeline(prompt, image=init_image, guidance_scale=3.0, strength=0.3).images[0]\n",
    "    display(make_image_grid([init_image, image.resize(init_image.size)], rows=1, cols=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDIM Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from diffusers import StableDiffusionPipeline, DDIMInverseScheduler, AutoencoderKL, DDIMScheduler, StableDiffusion3Pipeline\n",
    "from torchvision import transforms as tvt\n",
    "\n",
    "\n",
    "def load_image(imgname: str, target_size: Optional[Union[int, Tuple[int, int]]] = None) -> torch.Tensor:\n",
    "    pil_img = Image.open(imgname).convert('RGB')\n",
    "    if target_size is not None:\n",
    "        if isinstance(target_size, int):\n",
    "            target_size = (target_size, target_size)\n",
    "        pil_img = pil_img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    return tvt.ToTensor()(pil_img)[None, ...]  # add batch dimension\n",
    "\n",
    "\n",
    "def img_to_latents(x: torch.Tensor, vae: AutoencoderKL):\n",
    "    x = 2. * x - 1.\n",
    "    posterior = vae.encode(x).latent_dist\n",
    "    latents = posterior.mean * 0.18215 # sd < 3 \n",
    "    # latents = (posterior.mean - 0.0609) * 1.5305 # sd >= 3\n",
    "    return latents\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ddim_inversion(imgname: str, num_steps: int = 50, verify: Optional[bool] = False) -> torch.Tensor:\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    dtype = torch.float16\n",
    "    \n",
    "    # model_path = 'stabilityai/stable-diffusion-3.5-large'\n",
    "    model_path = 'stabilityai/stable-diffusion-2-1'\n",
    "\n",
    "    inverse_scheduler = DDIMInverseScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_path,\n",
    "                                                   scheduler=inverse_scheduler,\n",
    "                                                   safety_checker=None,\n",
    "                                                   torch_dtype=dtype)\n",
    "    pipe.to(device)\n",
    "    vae = pipe.vae\n",
    "\n",
    "    input_img = load_image(imgname).to(device=device, dtype=dtype)\n",
    "    # crop\n",
    "    input_img = input_img[:, :, :input_img.shape[-2], :512]\n",
    "    # height and width should be divisible by 8\n",
    "    input_img = input_img[:, :, :input_img.shape[-2] // 8 * 8, :input_img.shape[-1] // 8 * 8]\n",
    "    \n",
    "    latents = img_to_latents(input_img, vae)\n",
    "\n",
    "    inv_latents = pipe(prompt=\"a realistic image\", negative_prompt=\"\", guidance_scale=1.,\n",
    "                          width=input_img.shape[-1], height=input_img.shape[-2],\n",
    "                          output_type='latent', return_dict=False,\n",
    "                          num_inference_steps=num_steps, latents=latents)[0]\n",
    "\n",
    "    # verify\n",
    "    if verify:\n",
    "        pipe.scheduler = DDIMScheduler.from_pretrained(model_path, subfolder='scheduler')\n",
    "        image = pipe(prompt=\"\", negative_prompt=\"\", guidance_scale=1.,\n",
    "                     num_inference_steps=num_steps, latents=inv_latents)\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        \n",
    "        fig.set_figwidth(100)\n",
    "        fig.set_figheight(50)\n",
    "        \n",
    "        ax[0].imshow(tvt.ToPILImage()(input_img[0]))\n",
    "        ax[1].imshow(image.images[0])\n",
    "        plt.show()\n",
    "    return inv_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddim_inversion('/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1737465403.7203906/9.png', num_steps=999, verify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImageVariationPipeline\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "device = \"cuda\"\n",
    "sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n",
    "  \"lambdalabs/sd-image-variations-diffusers\",\n",
    "  revision=\"v2.0\",\n",
    "  )\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "\n",
    "im = Image.open('/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1737465403.7203906/9.png')\n",
    "crop_box = (0, 0, 512, im.height)\n",
    "im = im.crop(crop_box)\n",
    "\n",
    "tform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(\n",
    "        (224, 224),\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "        antialias=False,\n",
    "        ),\n",
    "    transforms.Normalize(\n",
    "      [0.48145466, 0.4578275, 0.40821073],\n",
    "      [0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "inp = tform(im).to(device).unsqueeze(0)\n",
    "\n",
    "out = sd_pipe(inp, guidance_scale=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "make_image_grid([im, out.images[0].resize(im.size)], rows=1, cols=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambdalabs/sd-image-variations-diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "url = '/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1737465403.7203906/9.png'\n",
    "init_image = load_image(url)\n",
    "crop_box = (0, 0, 512, init_image.height)\n",
    "init_image = init_image.crop(crop_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImageVariationPipeline\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = \"cuda:0\"\n",
    "sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n",
    "  \"lambdalabs/sd-image-variations-diffusers\",\n",
    "  revision=\"v2.0\",\n",
    "  )\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "\n",
    "tform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(\n",
    "        (224, 224),\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC,\n",
    "        antialias=False,\n",
    "        ),\n",
    "    transforms.Normalize(\n",
    "      [0.48145466, 0.4578275, 0.40821073],\n",
    "      [0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "inp = tform(init_image).to(device).unsqueeze(0)\n",
    "\n",
    "out = sd_pipe(inp, guidance_scale=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"images\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sayakpaul/FLUX.1-dev-edit-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "url = '/dtu/blackhole/00/215456/tta-vlm/benchmark_results/augmented_inputs_1737465403.7203906/9.png'\n",
    "init_image = load_image(url)\n",
    "crop_box = (0, 0, 512, init_image.height)\n",
    "init_image = init_image.crop(crop_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FluxControlPipeline, FluxTransformer2DModel\n",
    "from diffusers.utils import load_image\n",
    "import torch \n",
    "\n",
    "path = \"sayakpaul/FLUX.1-dev-edit-v0\" \n",
    "edit_transformer = FluxTransformer2DModel.from_pretrained(path, torch_dtype=torch.bfloat16)\n",
    "pipeline = FluxControlPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\", transformer=edit_transformer, torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "\n",
    "image = init_image\n",
    "print(image.size)\n",
    "\n",
    "prompt = \"make the image grayscale\"\n",
    "output_image = pipeline(\n",
    "    control_image=image,\n",
    "    prompt=prompt,\n",
    "    guidance_scale=30., # change this as needed.\n",
    "    num_inference_steps=50, # change this as needed.\n",
    "    max_sequence_length=512,\n",
    "    height=image.height,\n",
    "    width=image.width,\n",
    "    generator=torch.manual_seed(0)\n",
    ").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
